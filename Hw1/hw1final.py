# -*- coding: utf-8 -*-
"""Hw1Final

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sJZYHFEso88BTxLHM-Zb6kfK3SZL-aXh
"""

# Commented out IPython magic to ensure Python compatibility.
## Excercise Setup:

# Import packages:
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

import matplotlib.pyplot as plt

import torch
from torch import nn
from torch import optim
import torch.nn.functional as F
from torchvision import datasets, transforms, models

def getData():
  import sys
  import os  
  sys.stdout = open(os.devnull, 'w')
  # Apply normalization over imported images:
  transform = transforms.Compose([transforms.ToTensor(),
                                  transforms.Normalize((0.5), (0.5))])

  # Import Train and Test sets:
  trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)
  trainLoader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
  testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)
  testLoader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)
  
  sys.stdout = sys.__stdout__

  return trainset, trainLoader, testset, testLoader

# Define a Class for controlling the different regularization methods:
from enum import Enum, auto 

class RegMethod(Enum):
  No_Regularization   = auto()
  Droput              = auto()
  Weight_Decay        = auto()
  Batch_Normalization = auto()

# Implement and instantiate LeNet5 (Based on implementaition seen in https://github.com/ChawDoe/LeNet5-MNIST-PyTorch, with several modifications):
class LeNet5(nn.Module):
    def __init__(self, regMethod):
        super().__init__()
        batchNormMoment = 0.5
        self.regMethod = regMethod
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.batchNorm2d = nn.BatchNorm2d(16, momentum=batchNormMoment)
        self.fc1 = nn.Linear(256, 120)
        self.batchNorm1d_1 = nn.BatchNorm1d(120, momentum=batchNormMoment)
        self.fc2 = nn.Linear(120, 84)
        self.batchNorm1d_2 = nn.BatchNorm1d(84, momentum=batchNormMoment)
        self.fc3 = nn.Linear(84, 10)

        if regMethod == RegMethod.Droput:
          p = 0.2
        else:
          p = 0
        self.dropout = nn.Dropout(p)

    def forward(self, x):
        # x = x.view(1, -1, 28,28)
        x = self.dropout(F.max_pool2d(F.relu(self.conv1(x)), 2)) # Conv -> Relu -> Max Pooling
        x = self.dropout(F.max_pool2d(F.relu(self.conv2(x)), 2)) # Conv -> Relu -> Max Pooling
        if self.regMethod == RegMethod.Batch_Normalization:
          x = self.batchNorm2d(x)
        x = x.view(x.shape[0], -1) # Flattening
        x = self.dropout(F.relu(self.fc1(x))) # MLP -> Relu
        if self.regMethod == RegMethod.Batch_Normalization:
          x = self.batchNorm1d_1(x)
        x = self.dropout(F.relu(self.fc2(x))) # MLP -> Relu
        if self.regMethod == RegMethod.Batch_Normalization:
          x = self.batchNorm1d_2(x)
        x = self.dropout(F.relu(self.fc3(x))) # MLP -> Relu

        return F.log_softmax(x, dim=1) # Return the log-Likelihood distribution

    def classify(self, x):
        out = self.forward(x)        
        probs = torch.exp(out)
        _, preds = probs.topk(1, dim=1)
        return preds

## Success Counter:
import numpy as np

def getSuccesses(likelihood, labels):
  _, mlClass = likelihood.topk(1, dim=1)
  equals = mlClass == labels.view(*mlClass.shape)
  return mlClass.cpu().numpy(), np.count_nonzero(equals.cpu().numpy())

def netTrain(nEpochs, regMethodStr='No_Regularization', printProgress=True, saveWeights=True):

  try:
    regMethod = RegMethod[regMethodStr]
  except:
    raise Exception('Invalid Regularization Method!')
  # Initialize Data Container for saving the results: 
  import pandas as pd
  results = pd.DataFrame(columns=['Training', 'Test'], index=range(nEpochs))
  results.index.name = 'Epoch'

  losCriterion = nn.NLLLoss() # Loss criterion

  trainset, trainLoader, testset, testLoader = getData() # Get Data for training

  ## Train the Network:
  lenet5 = LeNet5(regMethod)  # Instantiation
  learningRate = 0.2
  if regMethod == RegMethod.Weight_Decay:
    weightDecay = 0.003
  else:
    weightDecay = 0

  if regMethod == RegMethod.No_Regularization:
    optimizer = optim.SGD(lenet5.parameters(), lr=0.2, weight_decay=weightDecay)
  else:
    optimizer = optim.Adam(lenet5.parameters(), lr=0.0008, weight_decay=weightDecay)

  trainLosses, testLosses, trainAccuracy, testAccuracy = [], [], [], []
  lenet5.to('cuda')
  for epoch in range(nEpochs):
      trainLoss = 0
      testLoss = 0
      trainSuccesses = 0
      testSuccesses = 0

      # Training:
      for images, labels in trainLoader: 
          images, labels = images.to('cuda'), labels.to('cuda')

          optimizer.zero_grad() # Set accumulated gradients to zero
          logLikelihood = lenet5.forward(images) # Forward-Pass
          # print(torch.exp(logLikelihood))
          # break
          batchLoss = losCriterion(logLikelihood, labels) # Apply Forward-Pass
          batchLoss.backward() # Back-Propogation
          optimizer.step() # Gradient Descent        

          trainLoss += batchLoss.item() # Calculate accumulative loss
          _, success = getSuccesses(torch.exp(logLikelihood), labels)
          trainSuccesses += success # Calculate accumulative Miss-Classifications

      # Validation:
      with torch.no_grad(): # Turn off gradient calculations for efficiency
          lenet5.eval() # Turn off Dropout/Batch Normalization parameters learning
          for images, labels in testLoader:
              images, labels = images.to('cuda'), labels.to('cuda')

              logLikelihood = lenet5.forward(images) # Forward-Pass

              testLoss += losCriterion(logLikelihood, labels).item()      
              _, success = getSuccesses(torch.exp(logLikelihood), labels)          
              testSuccesses += success # Calculate accumulative Miss-Classifications
          
          if regMethod == RegMethod.Droput: # Requires re-Evaluation w/o dropout
            trainSuccesses = 0
            for images, labels in trainLoader:
                images, labels = images.to('cuda'), labels.to('cuda')

                logLikelihood = lenet5.forward(images) # Forward-Pass

                trainLoss += losCriterion(logLikelihood, labels).item()      
                _, success = getSuccesses(torch.exp(logLikelihood), labels)          
                trainSuccesses += success # Calculate accumulative Miss-Classifications

      lenet5.train() # Restore Dropout/Batch Normalization parameters learning
      
      # Calculate and Save Results:
      trainLoss = trainLoss/len(trainLoader)
      testLoss = testLoss/len(testLoader)       
      trainAcc = trainSuccesses/len(trainset) * 100
      testAcc = testSuccesses/len(testset) * 100
      results.loc[epoch]['Training'] = trainAcc
      results.loc[epoch]['Test'] = testAcc

      if printProgress:
        print(f"\tEpoch #{epoch + 1}:\t Training Acc = {trainAcc}\t Test Acc = {testAcc}")
  
  if saveWeights:
      torch.save(lenet5.state_dict(), 'NetworksWeights.pkl')
  return lenet5, results

def netTest(weightsPath, regMethodStr):
  regMethod = RegMethod[regMethodStr]
  lenet5 = LeNet5(regMethod)
  lenet5.load_state_dict(torch.load(weightsPath))
  _, _, testset, testLoader = getData() # Get Data for testing
  lenet5.to('cuda')

  testSuccesses = 0
  with torch.no_grad(): # Turn off gradient calculations for efficiency
      lenet5.eval()
      for images, labels in testLoader:
          images, labels = images.to('cuda'), labels.to('cuda')

          logLikelihood = lenet5.forward(images) # Forward-Pass
          _, success = getSuccesses(torch.exp(logLikelihood), labels)          
          testSuccesses += success # Calculate accumulative Miss-Classifications            
      testAcc = testSuccesses/len(testset) * 100

  print(f"Test Acc = {testAcc}")
      
  return testAcc

def plotResults(results):
  plt.figure()
  plt.plot(results['Training'], label='Training')
  plt.plot(results['Test'], label='Test')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy[%]')
  plt.grid()
  plt.legend()

def InferenceVal(lenet5):  
  _,_,testset, testLoader = getData()
  ## Inference validation:
  images, labels = next(iter(testLoader))
  images, labels = images.to('cuda'), labels.to('cuda')
  with torch.no_grad():
    logLikelihood = lenet5.forward(images)
  mlClasses, testSuccesses = getSuccesses(torch.exp(logLikelihood), labels)
  imgIdx = torch.randint(len(testLoader))
  plt.imshow(images[imgIdx].cpu().view(28, 28))

  ax = plt.title(f"Ground-Trooth: {testset.classes[labels[imgIdx]]}. Classified as {testset.classes[mlClasses[imgIdx][0]]}")
  plt.axis('off')

# lenet5, results = train(10, regMethodStr='Batch_Normalization')

# torch.save(lenet5.state_dict(), 'NetworksWeights.pkl')

# test('NetworksWeights.pkl', 'Batch_Normalization')

if __name__ == '__main__':
  import sys
  if sys.argv[1] == 'Train':
    nEpochs = int(sys.argv[2])
    regMethodStr = sys.argv[3]
    printProgress = True if sys.argv[4]=='True' else False
    saveWeights = True if sys.argv[5]=='True' else False
    lenet5, results = netTrain(nEpochs, regMethodStr, printProgress, saveWeights)
  
  elif sys.argv[1] == 'Test':
    weightsPath = sys.argv[2]
    regMethodStr = sys.argv[3]
    testAcc = netTest(weightsPath, regMethodStr)

  else:
    raise Exception('Invalid request!')
